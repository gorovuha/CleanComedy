{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff3c8b-39f1-4002-b4a2-cda80c2d0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from detoxify import Detoxify\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089ea96-48b8-4f5f-b00d-6bc897738d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "df_test = pd.DataFrame(list(data.items()), columns=['text', 'label'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67587c-81f0-4b2f-98d1-ea34f8ed3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "df_train = pd.DataFrame(list(data.items()), columns=['text', 'label'])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204fba5-96df-45d9-9144-b76e214ff6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test])\n",
    "df.to_csv('all_russian.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069913b3-206a-46a0-9800-61473ba9a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "funny_texts = df[df['label'] == 1]['text'].tolist()              #choosing only jokes\n",
    "len(funny_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c942c3-00ce-4972-9d13-c802323398c9",
   "metadata": {},
   "source": [
    "# Detoxify multilingual (XLM RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79484d-7ad1-4ba1-8f22-38e152540402",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Detoxify('multilingual', device=device)\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "toxicity_scores_df = pd.DataFrame()\n",
    "\n",
    "for chunk in chunker(funny_texts, 10000):\n",
    "    results = model.predict(chunk)\n",
    "    chunk_df = pd.DataFrame(results)\n",
    "    toxicity_scores_df = pd.concat([toxicity_scores_df, chunk_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f83a2-9575-4d01-85b0-4cc8865e520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec34291-f49e-4af4-b074-227eed9e962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_scores_df.to_csv('toxicity_scores_detoxify.csv', index=False)      #file comtaining different toxicity labels Detoxify predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d7f8c8-516f-4614-af43-2c2347cba94d",
   "metadata": {},
   "source": [
    "# ruBERT toxic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cb3b5-0151-4d25-b556-5b2135d2b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"IlyaGusev/rubertconv_toxic_clf\"\n",
    "pipe = pipeline(\"text-classification\", model=model_name, tokenizer=model_name, framework=\"pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773eace5-227f-4bfc-a3e1-0105486d9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "funny_texts = df[df['label'] == 1]['text'].tolist()\n",
    "result = pipe(funny_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5a002-2f63-4714-8ca3-aca604b507d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed454b-a703-459f-980e-dc3783dd523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tox_results.txt', 'w') as f:          #file containing ruBERT predictions\n",
    "    f.write(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27ce59-02a3-40ee-bd9a-5fb8daac30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tox_results.txt') as file:\n",
    "    data_string = file.read()\n",
    "    scores_cls = ast.literal_eval(data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113cad2-73ad-4305-a2d1-431f0d37eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c198cf-b191-4827-bdd7-65e2815c49b5",
   "metadata": {},
   "source": [
    "## Toxicity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1d44f-7529-4816-a19b-43e99113614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tox_results.txt') as file:\n",
    "    data_string = file.read()\n",
    "    scores_cls = ast.literal_eval(data_string)\n",
    "df = pd.DataFrame(scores_cls)\n",
    "df.to_csv('tox_results.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a82995-fab9-4844-8262-15d3769968d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_russian.csv')       #FUN dataset train+test only jokes\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a32b5-b775-41a8-b05c-873c1c41b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()\n",
    "dataset['text'] = df['text']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21b88c-5a3a-4aa9-81be-f4fc93ec95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tox_results.txt') as file:               #ruBERTConv Toxicity Classifier predictions\n",
    "    data_string = file.read()\n",
    "    scores_cls = ast.literal_eval(data_string)\n",
    "df = pd.DataFrame(scores_cls)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916577f-6a6e-4981-94f6-48a3fb892e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(df)):\n",
    "    if df['label'][i] == 'toxic':\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "dataset['rubert'] = labels\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bedd9-e156-4a27-a5dd-35b9a19460f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('toxicity_scores_detoxify.csv')           #Multilingual Detoxify (XLM RoBERTa) toxicity predictions\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24d3bc-5760-461f-9b08-4cbadf729d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm = []\n",
    "for i in range(len(df)):\n",
    "    xlm.append(df['toxicity'][i])\n",
    "\n",
    "dataset['xlm'] = xlm\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497aa156-375d-4cf4-98f6-dd0d17be07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('compare_tox.csv')           #comparable table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26d9ee-6f97-4096-a660-94ce2750b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('compare_tox.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8dbfda-d2f7-4896-ab11-648a1ed61640",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_i = []\n",
    "for i in range(len(df)):\n",
    "    if df['rubert'][i] == 1 or df['xlm'][i] >= 0.1:               #deleting all toxic content \n",
    "        drop_i.append(i)\n",
    "filt_df = df.drop(drop_i)\n",
    "filt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cee930-2972-42f8-a398-eec89ab3ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df.to_csv('rus_filtered.csv')            #detoxified Russian jokes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade517d6-d214-4e6a-bf4b-60229352087a",
   "metadata": {},
   "source": [
    "# Deduplication process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec54e3c-80d4-4766-b57d-a793f64f46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"rus_filtered.csv\")        #read results from the previous step\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025457d9-6c56-42ab-ba3c-4ee773f2727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "def product(x, y=None, batch_size=None):\n",
    "    if y is None:\n",
    "        y = x\n",
    "    if batch_size is None:\n",
    "        return x.dot(y.T)\n",
    "    result = np.zeros((x.shape[0], y.shape[0]), dtype='float16')\n",
    "    for i in trange((len(x) + batch_size - 1) // batch_size):\n",
    "        left, right = i * batch_size, (i + 1) * batch_size\n",
    "        result[left:right] = x[left:right].dot(y.T)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c5e50-54e0-4112-91e1-5fbd8a0030c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a1958-95c7-4675-81d3-4b665c46e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = SentenceTransformer(model=model, device=device)\n",
    "    jokes = list(dataset['text'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = []\n",
    "        for i in trange((len(jokes) + BATCH_SIZE - 1) // BATCH_SIZE):\n",
    "            batch = jokes[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "            x.append(model.encode(batch))\n",
    "    model, dataset, jokes, batch = None, None, None, None\n",
    "\n",
    "    x = np.concatenate(x)\n",
    "    x /= np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    x = product(x, batch_size=BATCH_SIZE)                                                #cosine similairy calculation\n",
    "    indexes_1, indexes_2 = np.where((x >= 0.923) & (~np.tri(len(x), dtype=bool)))        #threshold 0.7, adjust if necessary\n",
    "    x = x[indexes_1, indexes_2]\n",
    "    data = pd.DataFrame({'indexes_1': indexes_1, 'indexes_2': indexes_2, 'cos': x})          \n",
    "    data.to_csv('sbert_duplicates_ru.csv', index=False)                                        #file containing indexes of duplicating jokes\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da7c5b-2754-4495-8a55-7bf440a204a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_drop = []\n",
    "ind_keep = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['indexes_1'][i] in ind_keep:\n",
    "        if data['cos'][i] >= 0.923:                       #from set of duplicating jokes with cosine similarity higher than 0.923, \n",
    "            ind_drop.append(data['indexes_2'][i])         #we leave only first joke\n",
    "            ind_keep.append(data['indexes_2'][i])\n",
    "    else:\n",
    "        ind_keep.append(data['indexes_1'][i])\n",
    "        ind_drop.append(data['indexes_2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e580470-b626-458b-b478-8f721eb27127",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(ind_drop))                          #the number of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ffdd9-b7db-4722-a0c3-98e187e532f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(ind_drop)\n",
    "dataset.to_csv('dataset_without_dups_ru.csv', index=False)                #dataset without duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1e244-5630-4511-9128-48195f6b294a",
   "metadata": {},
   "source": [
    "# Sentiment analysis #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1072a-d990-4ccf-94a8-dac1cb111375",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "model = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"           #zero-shot classifier for sentiment analysis\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('rus_filtered_without_dup_test.tsv', delimiter='\\t')\n",
    "    #data['full_joke'] = data['set-up'] + ' ' + data['punchline']\n",
    "    data['label'] = ['' for i in range(len(data))]\n",
    "\n",
    "    labels = []\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=model, device=device)\n",
    "    candidate_labels = [\"politics\", \"neutral\", \"offending\", \"alcohol\", \"racist\", \"drugs\"]\n",
    "    print('model is ready')\n",
    "    for i in trange((len(data['text']) + BATCH_SIZE - 1) // BATCH_SIZE):\n",
    "        inputs = list(data.iloc[i*BATCH_SIZE:(i+1)*BATCH_SIZE]['text'])\n",
    "        outputs = classifier(inputs, candidate_labels, multi_label=False)\n",
    "        labels.append([output['labels'][np.argmax(output['scores'])] for output in outputs])\n",
    "\n",
    "    data['label'] = np.concatenate(labels)\n",
    "    data.to_csv('labeled_dataset_without_dups_ru.csv', index=False)        #file containing deduplicated jokes with labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce18e62-ae4e-4c49-ade3-661af7b0dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_i = []\n",
    "data = pd.read_csv('labeled_dataset_without_dups_ru.csv')          #read previous stage result or comment this line\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['label'][i] == 'politics':                             #removing inappropriate jokes\n",
    "        drop_i.append(i)\n",
    "    if data['label'][i] == 'offending':\n",
    "        drop_i.append(i)\n",
    "    if data['label'][i] == 'drugs':\n",
    "        drop_i.append(i)\n",
    "    if data['label'][i] == 'alcohol':\n",
    "        drop_i.append(i)\n",
    "data = data.drop(drop_i)\n",
    "data.to_csv('filtered_ru.csv', index=False)                        #filtered jokes\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcd960-2476-4bfc-80b6-59bff1a36db3",
   "metadata": {},
   "source": [
    "# Topic modeling #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99be47-defd-48d8-a201-5630195b4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9520ca5-f86b-4b58-85ed-c55f1fcba3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = KeyBERTInspired()\n",
    "cluster_model = KMeans(n_clusters=100)             #choosing modeling algorithm \n",
    "topic_model = BERTopic(\"russian\", verbose=True, representation_model=representation_model, hdbscan_model=cluster_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70cb44-7ebe-483a-b880-c8c5e987b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('filtered_ru.csv')                 #reading previous stage results\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc5caa-932c-4b18-878c-6ebce338491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(df['text'].to_list())        #fitting BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760a243-c630-4757-985b-dd8e7278190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "freq.to_csv('BERTopik_ru_kmeans_100_.csv')         #file containing 100 topics with examples and key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc61d8-28f3-417d-8c12-75084d5b4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794ee0f-72d2-4709-a1f0-c846d3020fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default='iframe'\n",
    "topic_model.visualize_topics().show()                    #the bubble clusters representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14ee45-d418-4bb7-b659-927a3b06d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "docs = df['text'].to_list()\n",
    "sentence_model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad998692-86a5-4528-898a-a28b3f2c4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "import spacy\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")                                                      #another clusters representation\n",
    "translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.1, metric='cosine')\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "document_info = topic_model.get_document_info(docs)\n",
    "topic_sizes = topic_model.get_topic_freq()\n",
    "\n",
    "centroids = {}\n",
    "for topic in topic_sizes.Topic:\n",
    "    indices = document_info[document_info.Topic == topic].index\n",
    "    centroids[topic] = np.mean(reduced_embeddings[indices], axis=0)\n",
    "\n",
    "topic_labels = topic_model.get_topic_info()\n",
    "\n",
    "unique_topics = document_info.Topic.unique()\n",
    "topic_to_color = {topic: idx for idx, topic in enumerate(unique_topics)}\n",
    "colors = [topic_to_color[topic] for topic in document_info.Topic]\n",
    "\n",
    "def lemmatize_and_translate(label):\n",
    "    try:\n",
    "        doc = nlp(label.split('_')[0])\n",
    "        lemmatized_label = doc[0].lemma_\n",
    "        #print(f\"Lemmatized label: {lemmatized_label}\")\n",
    "        translated_label = translator.translate(lemmatized_label)\n",
    "        #print(f\"Translated label: {translated_label}\")\n",
    "        return f\"{lemmatized_label} ({translated_label})\"\n",
    "    except Exception as e:\n",
    "        #print(f\"Error translating label '{label}': {e}\")\n",
    "        return label\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "scatter = ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], \n",
    "                     c=colors, cmap='Spectral', s=2, alpha=0.5)\n",
    "\n",
    "texts = []\n",
    "for topic, size in topic_sizes.itertuples(index=False):\n",
    "    if size > 300:  # Показывать аннотации только для крупных тем\n",
    "        cluster_center = centroids[topic]\n",
    "        topic_label = topic_labels[topic_labels.Topic == topic].Name.values[0].split('_')[1]  # Берем только первое слово\n",
    "        annotated_label = lemmatize_and_translate(topic_label)\n",
    "        texts.append(ax.text(cluster_center[0], cluster_center[1], annotated_label, fontsize=10, ha='center', va='center', \n",
    "                             bbox=dict(\n",
    "                                 facecolor='white', alpha=0.6, edgecolor='black', boxstyle='round,pad=0.5')))\n",
    "\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='black'))\n",
    "\n",
    "ax.set_title(\"UMAP projection of BERTopic clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7773b4-e1f9-43e8-9d2b-c2e4164f1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\"topic_model\", serialization=\"pickle\")      #saving our topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e027729-1e90-4966-9b08-f791a01c4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = topic_model.get_document_info(df['text'].to_list())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea97c3-1cb7-4812-9d16-d1c281c3e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_topics = [2, 4, 6, 10, 12, 13, 17, 28, 29, 38, 51, 58, 59, 76, 78, 87, 90, 94, ]\n",
    "drop_index = []                  #after reading cluster names, removing jokes from inappropriate ones\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df['Topic'][i] in drop_topics:\n",
    "        drop_index.append(i)\n",
    "df = df.drop(drop_index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84d043-4650-4251-95ed-859a329ad955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_comedy_ru.csv', index=False)                      #final cleared dataset "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
