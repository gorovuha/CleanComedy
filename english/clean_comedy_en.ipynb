{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6a0bf-39fe-4724-8d80-7a5b370a22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==2.1.4\n",
    "!pip install numpy==1.26.4\n",
    "!pip install torch==2.0.1\n",
    "!pip install transformers==4.38.2\n",
    "!pip install sentence-transformers==2.5.1\n",
    "!pip install detoxify==0.5.2\n",
    "!pip install bertopic==0.16.2\n",
    "!pip install scikit-learn==1.5.0\n",
    "!pip install plotly==5.22.0\n",
    "!pip install matplotlib==3.9.0\n",
    "!pip install umap==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03735be-f307-4876-8adc-ed92828edb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c575ea-b706-4bb9-85b9-be3fa44f6f89",
   "metadata": {},
   "source": [
    "# Initial preprocessing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278387a-e0f9-4e6c-91c8-d2e6d336d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_PATH = 'raw_data'\n",
    "GLOBAL_PATH = os.path.join(os.getcwd(), LOCAL_PATH)\n",
    "MIN_LENGTH = 30\n",
    "MAX_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc4f80-60c5-47f3-abac-c947bd3d6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_and_drop_duplicates():\n",
    "    print('Concatenation...', end=' ')\n",
    "    full_df = None\n",
    "    for name in os.listdir(LOCAL_PATH):\n",
    "        file_path = os.path.join(GLOBAL_PATH, name)\n",
    "        if name.endswith('txt'):\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                df = pd.DataFrame({'text': [line.strip() for line in f.readlines()[1:]]})\n",
    "        else:\n",
    "            df = pd.read_csv(file_path).drop(columns='humor_rating', errors='ignore')\n",
    "            raw_text = df.get('text')\n",
    "            if raw_text is None:\n",
    "                raw_text = df.get('title') + ' ' + df.get('selftext')\n",
    "                df['text'] = raw_text\n",
    "            raw_text = raw_text.str.lower().replace('\\W+', ' ', regex=True).str.strip()\n",
    "            df['raw_text'] = raw_text\n",
    "        full_df = pd.concat((full_df, df), ignore_index=True)\n",
    "    print('End...')\n",
    "    df = full_df.drop_duplicates(subset='raw_text', ignore_index=True).replace('\\s+', ' ', regex=True)\n",
    "    print(f'{len(df)} examples')\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_bad_examples(df):          #choosing jokes fitting common structure\n",
    "    initial_len = len(df)\n",
    "    print('Removing examples with symbols not in [^a-zA-Z ?!.,;:\"\\']...', end=' ')\n",
    "    mask = ~df.text.str.contains('[^a-zA-Z ?!.,;:\"\\']', na=True)\n",
    "    df = df[mask].reset_index(drop=True)\n",
    "    print('End...')\n",
    "    print(f'{len(df)}/{initial_len} examples left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def sorting_by_length(df, column_name='raw_text', min_length=None, max_length=None):\n",
    "    initial_len = len(df)\n",
    "    print('Sorting by lengths...', end=' ')\n",
    "    sorted_lengths = df.get(column_name).str.len().sort_values().dropna()\n",
    "    if min_length is None:\n",
    "        min_length = sorted_lengths.min()\n",
    "    if max_length is None:\n",
    "        max_length = sorted_lengths.max()\n",
    "    sorted_lengths = sorted_lengths[(sorted_lengths >= min_length) & (sorted_lengths <= max_length)]\n",
    "    df = df.iloc[sorted_lengths.index].reset_index(drop=True)\n",
    "    print('End...')\n",
    "    print(f'{len(df)}/{initial_len} examples left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_setup_and_punchline(df):                                          #set-up -- punchline division\n",
    "    initial_len = len(df)\n",
    "    not_divided = (df.title != df.title) & (df.selftext != df.selftext)\n",
    "    print(f'Start: {sum(not_divided)}/{len(df)} examples are not divided')\n",
    "    text = df.text.str.rstrip('?!.:;, ')\n",
    "    symbols = ['?', '!', '.', '[!?.]+', ':', ';', ',']\n",
    "    for symbol in symbols:\n",
    "        split = text.str.split(symbol)\n",
    "        mask = not_divided & (split.str.len() == 2)\n",
    "        not_divided = not_divided & (split.str.len() != 2)\n",
    "        print(f'After split by \"{symbol}\": {sum(not_divided)} examples are not divided yet')\n",
    "        split = df[mask].text.str.split(symbol)\n",
    "        symbol = symbol if len(symbol) == 1 else '.'\n",
    "        df.loc[mask, 'title'] = split.map(lambda x: x[0].strip() + symbol)\n",
    "        df.loc[mask, 'selftext'] = split.map(lambda x: x[1].strip())\n",
    "    df = df.rename(columns={'title': 'set-up', 'selftext': 'punchline'}).dropna(subset=['set-up', 'punchline'])\n",
    "    df = df.loc[:, ['score', 'set-up', 'punchline']].sample(frac=1).reset_index(drop=True)\n",
    "    df = df[df.punchline.str.len() > 4].reset_index(drop=True)\n",
    "    print(f'{len(df)}/{initial_len} examples left')\n",
    "    return df\n",
    "\n",
    "\n",
    "#dataset = concatenate_and_drop_duplicates()\n",
    "dataset = pd.read_csv('all_data.csv')                                 #concatenated english humour datasets\n",
    "dataset = delete_bad_examples(dataset)\n",
    "dataset = sorting_by_length(dataset, min_length=MIN_LENGTH, max_length=MAX_LENGTH)\n",
    "dataset = get_setup_and_punchline(dataset)\n",
    "dataset.to_csv('initially_preprocessed_data_en.csv', index=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb3663f-0526-4a30-8284-7e6a4e95ab6f",
   "metadata": {},
   "source": [
    "# Deduplication process #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06290f9-d099-4ab0-85cc-bebb97aa204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv('initially_preprocessed_data_en.csv')    #read the previous stage result\n",
    "dataset['full_joke'] = dataset['set-up'] + \" \" + dataset['punchline']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468c45d-e22b-48cd-bdbc-7cd040647d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "def product(x, y=None, batch_size=None):\n",
    "    if y is None:\n",
    "        y = x\n",
    "    if batch_size is None:\n",
    "        return x.dot(y.T)\n",
    "    result = np.zeros((x.shape[0], y.shape[0]), dtype='float16')\n",
    "    for i in trange((len(x) + batch_size - 1) // batch_size):\n",
    "        left, right = i * batch_size, (i + 1) * batch_size\n",
    "        result[left:right] = x[left:right].dot(y.T)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cccdb-437b-4d50-9404-2a4ef37a0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'  # choose your device (cpu, cuda, mps)\n",
    "model = 'all-mpnet-base-v2'  # Sentence BERT model\n",
    "\n",
    "model = SentenceTransformer(model, device=device)  # choose your device (cpu, cuda, mps)\n",
    "jokes = list(dataset['full_joke'].replace(to_replace=r'[^\\w\\s]', value='', regex=True).str.lower())\n",
    "with torch.no_grad():\n",
    "    x = []\n",
    "    for i in trange((len(jokes) + BATCH_SIZE - 1) // BATCH_SIZE):\n",
    "        batch = jokes[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        x.append(model.encode(batch))\n",
    "model, dataset, jokes, batch = None, None, None, None\n",
    "\n",
    "x = np.concatenate(x)\n",
    "x /= np.linalg.norm(x, axis=1, keepdims=True)\n",
    "x = product(x, batch_size=BATCH_SIZE)  # cosine similairy calculation\n",
    "indexes_1, indexes_2 = np.where((x >= 0.7) & (~np.tri(len(x), dtype=bool)))  # threshold 0.7, adjust if necessary\n",
    "x = x[indexes_1, indexes_2]\n",
    "data = pd.DataFrame({'indexes_1': indexes_1, 'indexes_2': indexes_2, 'cos': x})\n",
    "data.to_csv('sbert_duplicates_en.csv', index=False)  # file containing indexes of duplicating jokes\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c10bfd-95f7-4623-9df0-a967a0c009d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_drop = []\n",
    "ind_keep = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['indexes_1'][i] in ind_keep:\n",
    "        if data['cos'][i] > 0.7:  # from set of duplicating jokes with cosine similarity higher than 0.7, \n",
    "            ind_drop.append(data['indexes_2'][i])  # we leave only first joke\n",
    "    else:\n",
    "        ind_keep.append(data['indexes_1'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ddce1-1fc8-4889-a490-221577353eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(ind_drop))  # the number of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b22ab-d90d-4bad-b7ee-5f82db1a1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(ind_drop)\n",
    "dataset.to_csv('dataset_without_dups_en.csv', index=False)  # dataset without duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed98da-ca26-425d-bf92-664ec706567f",
   "metadata": {},
   "source": [
    "# Sentiment analysis #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5af5a6-20fd-4c33-b0e4-55aff95f305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "device = 'cuda'  # choose your device (cpu, cuda, mps)\n",
    "model = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"  # zero-shot classifier for sentiment analysis\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('dataset_without_dups_en.csv')\n",
    "    #data = dataset                                    \n",
    "    data['label'] = ['' for i in range(len(data))]\n",
    "    \n",
    "    labels = []\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=model, device=device)\n",
    "    candidate_labels = [\"politics\", \"neutral\", \"racist\", \"offending\", \"drugs\", \"alcohol\"]          \n",
    "    print('model is ready')\n",
    "    for i in trange((len(data['full_joke']) + BATCH_SIZE - 1) // BATCH_SIZE):\n",
    "        inputs = list(data.iloc[i*BATCH_SIZE:(i+1)*BATCH_SIZE]['full_joke'])\n",
    "        outputs = classifier(inputs, candidate_labels, multi_label=False)\n",
    "        labels.append([output['labels'][np.argmax(output['scores'])] for output in outputs])\n",
    "\n",
    "    data['label'] = np.concatenate(labels)\n",
    "    data.to_csv('labeled_dataset_without_dups_en.csv', index=False)  # file containing deduplicated jokes with labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dad6f7-453a-49e8-97ce-5cdb724d0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_i = []\n",
    "data = pd.read_csv('labeled_dataset_without_dups_en.csv')  # read previous stage result or comment this line\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['label'][i] == 'politics':  # removing inappropriate jokes\n",
    "        drop_i.append(i)\n",
    "    if data['label'][i] == 'offending':\n",
    "        drop_i.append(i)\n",
    "    if data['label'][i] == 'racist':\n",
    "        drop_i.append(i)\n",
    "    if data['label'][i] == 'drugs':\n",
    "        drop_i.append(i)\n",
    "    if data['label'][i] == 'alcohol':\n",
    "        drop_i.append(i)\n",
    "data = data.drop(drop_i)\n",
    "data.to_csv('filtered_en.csv', index=False)  # filtered jokes\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c0b9fd-ffa0-4449-b2ee-7c336b9127dd",
   "metadata": {},
   "source": [
    "# Topic modeling #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3ec93-3d6c-4ef5-8db3-0a45a7ecb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e04bb9-828d-4619-81ec-5a79b0ad3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = KeyBERTInspired()\n",
    "cluster_model = KMeans(n_clusters=100)  # choosing modeling algorithm \n",
    "topic_model = BERTopic(\"english\", verbose=True, representation_model=representation_model, hdbscan_model=cluster_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10438e75-4516-4858-b0c5-9a67279ed4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('filtered_en.csv')         #reading previous stage results\n",
    "#df = data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0033df-e961-435f-a4cf-58ed3deabd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(df['full_joke'].to_list())  # fitting BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cd572-f60e-4a0c-afe7-cc5a8a8640bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "freq.to_csv('BERTopik_eng_kmeans_100.csv')  # file containing 100 topics with examples and key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ce69e-9447-4a0a-93eb-52f1bae86066",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210819d-7ac1-4dc8-a108-096012f6cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default='iframe'\n",
    "topic_model.visualize_topics().show()  # the bubble clusters representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d4753-8973-4e6e-b881-a14f99284f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['full_joke'].to_list()\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8182d4a1-485e-4e75-88a3-9d0e774f5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.1, metric='cosine')  # another clusters representation\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "document_info = topic_model.get_document_info(docs)\n",
    "topic_sizes = topic_model.get_topic_freq()\n",
    "\n",
    "centroids = {}\n",
    "for topic in topic_sizes.Topic:\n",
    "    indices = document_info[document_info.Topic == topic].index\n",
    "    centroids[topic] = np.mean(reduced_embeddings[indices], axis=0)\n",
    "\n",
    "topic_labels = topic_model.get_topic_info()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "scatter = ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], \n",
    "                     c=[topic_to_color[topic] for topic in document_info.Topic], \n",
    "                     cmap='Spectral', s=2, alpha=0.5)\n",
    "\n",
    "texts = []\n",
    "for topic, size in topic_sizes.itertuples(index=False):\n",
    "    if size > 100:  # showing labels only for big clusters\n",
    "        cluster_center = centroids[topic]\n",
    "        topic_label = topic_labels[topic_labels.Topic == topic].Name.values[0].split('_')[1]  # only forst word from cluster name\n",
    "        texts.append(ax.text(cluster_center[0], cluster_center[1], topic_label, fontsize=10, ha='center', va='center', \n",
    "                             bbox=dict(facecolor='white', alpha=0.6, edgecolor='black', boxstyle='round,pad=0.5')))\n",
    "\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='black'))\n",
    "\n",
    "ax.set_title(\"BERTopic clusters of CleanComedy English\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1db8f9-9b14-475a-8f13-47c53c69de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\"topic_model_en\", serialization=\"pickle\")  # saving our topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79125142-63de-4cd5-b19f-4b964d910fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_topics = [96, 83, 70, 38, 34, 33, 19, 6, 2]  # after reading cluster names, removing jokes from inappropriate ones\n",
    "drop_index = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df['Topic'][i] in drop_topics:\n",
    "        drop_index.append(i)\n",
    "df = df.drop(drop_index)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
